# technical_analyzer.py
# –í–µ—Ä—Å–∏—è: 2.7 (–§–∏–Ω–∞–ª—å–Ω–∞—è: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è DataFrame)

import gspread
from google.oauth2.service_account import Credentials
import pandas as pd
import pandas_ta as ta
from datetime import datetime
import logging
from typing import Dict, List, Any, Optional

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥—É–ª—è –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("analyzer.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def get_worksheet(sheet_name: str) -> Optional[gspread.Worksheet]:
    """–ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Google Sheets –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç –ª–∏—Å—Ç–∞."""
    try:
        creds = Credentials.from_service_account_file('credentials.json', scopes=['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive.file'])
        client = gspread.authorize(creds)
        spreadsheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1qBYS_DhGNsTo-Dnph3g_H27aHQOoY0EOcmCIKarb7Zc/")
        return spreadsheet.worksheet(sheet_name)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –ª–∏—Å—Ç—É '{sheet_name}': {e}")
        return None

def calculate_indicators_and_state(df_for_calc: pd.DataFrame, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –ß–ò–°–¢–û–ú DataFrame (—Ç–æ–ª—å–∫–æ OHLC).
    """
    if df_for_calc.shape[0] < 50:
        return {}

    df_for_calc.ta.rsi(length=14, append=True)
    df_for_calc.ta.sma(length=20, append=True)
    df_for_calc.ta.sma(length=50, append=True)
    df_for_calc.ta.bbands(length=20, append=True)
    latest = df_for_calc.iloc[-1]

    rsi = latest.get('RSI_14')
    state = "Neutral"
    recommendation = "-"
    if pd.notna(rsi):
        warning_level = float(str(config.get('RSI_WARNING_LEVEL', 35)).replace(',', '.'))
        alert_level = float(str(config.get('RSI_ALERT_LEVEL', 30)).replace(',', '.'))
        proximity_pct = float(str(config.get('PROXIMITY_PERCENTAGE', 15)).replace(',', '.'))
        proximity_start_level = warning_level * (1 + proximity_pct / 100)
        if rsi < alert_level:
            state = "Oversold"
            recommendation = "Alert Sent"
        elif rsi < warning_level:
            state = "Warning"
            recommendation = "Monitor for reversal"
        elif rsi < proximity_start_level:
            state = "Proximity"
            recommendation = "Add to Hotlist?"

    def format_number(num: Optional[float]) -> str:
        if pd.notna(num):
            return f'{num:,.2f}'.replace(',', ' ').replace('.', ',')
        return 'N/A'

    return {
        'State': state, 'Recommendation': recommendation,
        'RSI_14': format_number(latest.get('RSI_14')),
        'MA_20': format_number(latest.get('SMA_20')),
        'MA_50': format_number(latest.get('SMA_50')),
        'BB_Upper': format_number(latest.get('BBU_20_2.0')),
        'BB_Lower': format_number(latest.get('BBL_20_2.0')),
    }

def main_analyzer() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: —á–∏—Ç–∞–µ—Ç –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É."""
    logging.info("\n" + "="*50)
    logging.info(f"--- üß† ASIPM-AI: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä v2.7 (–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π) üß† ---")
    logging.info("="*50)
    sheets = {name: get_worksheet(name) for name in ['History_OHLCV', 'Analysis', 'Config']}
    if not all(sheets.values()):
        logging.critical("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –æ–¥–Ω–æ–º—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ª–∏—Å—Ç–∞–º Google. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return

    logging.info("üîÑ –ß–∏—Ç–∞—é –∫–æ–Ω—Ñ–∏–≥–∏ –∏ –í–°–Æ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞...")
    history_records = sheets['History_OHLCV'].get_all_records()
    configs_raw = sheets['Config'].get_all_records()
    config = {item['Parameter']: item['Value'] for item in configs_raw}

    if not history_records:
        logging.warning("–õ–∏—Å—Ç 'History_OHLCV' –ø—É—Å—Ç. –ê–Ω–∞–ª–∏–∑ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        return

    # --- –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞–Ω–∏—è DataFrame –∏–∑ get_all_records() ---
    history_df = pd.DataFrame(history_records)
    # ------------------------------------------------------------------------------------

    cols_to_process = ['Open', 'High', 'Low', 'Close', 'Volume']
    for col in cols_to_process:
        if col in history_df.columns:
            history_df[col] = pd.to_numeric(history_df[col], errors='coerce')

    assets_to_analyze = history_df[['Ticker', 'Timeframe']].drop_duplicates().to_dict('records')
    logging.info(f"‚òëÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(assets_to_analyze)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä (—Ç–∏–∫–µ—Ä/—Ç–∞–π–º—Ñ—Ä–µ–π–º) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

    all_analysis_results: List[List[Any]] = []
    for asset_key in assets_to_analyze:
        ticker, timeframe = asset_key['Ticker'], asset_key['Timeframe']
        logging.info(f"  - –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {ticker} –Ω–∞ {timeframe}...")

        ticker_history = history_df[(history_df['Ticker'] == ticker) & (history_df['Timeframe'] == timeframe)].copy()
        if ticker_history.empty: continue

        ticker_history['Date'] = pd.to_datetime(ticker_history['Date'])
        ticker_history.sort_values(by='Date', inplace=True)

        columns_for_calc = ['Open', 'High', 'Low', 'Close']
        if not all(col in ticker_history.columns for col in columns_for_calc):
            logging.warning(f"    - ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é {ticker}, —Ç.–∫. –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã OHLC.")
            continue
        
        calculation_df = ticker_history[columns_for_calc].copy().dropna()
        
        analysis_result = calculate_indicators_and_state(calculation_df, config)
        if not analysis_result:
            logging.warning(f"    - ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ {ticker} –Ω–∞ {timeframe} (< 50 —Å–≤–µ—á–µ–π).")
            continue

        new_row = [
            ticker, timeframe, analysis_result.get('State'),
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), analysis_result.get('RSI_14'),
            analysis_result.get('MA_20'), analysis_result.get('MA_50'),
            analysis_result.get('BB_Upper'), analysis_result.get('BB_Lower'),
            "N/A", analysis_result.get('Recommendation')
        ]
        all_analysis_results.append(new_row)
        logging.info(f"    - ‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–æ—Å—Ç–æ—è–Ω–∏–µ: {analysis_result.get('State')}, RSI: {analysis_result.get('RSI_14')}")

    if all_analysis_results:
        logging.info(f"\nüîÑ –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é –ª–∏—Å—Ç 'Analysis' {len(all_analysis_results)} —Å—Ç—Ä–æ–∫–∞–º–∏...")
        try:
            headers = ['Ticker', 'Timeframe', 'State', 'Last_Update', 'RSI_14', 'MA_20', 'MA_50', 'BB_Upper', 'BB_Lower', 'Pattern_Found', 'Recommendation']
            sheets['Analysis'].clear()
            sheets['Analysis'].update(range_name='A1', values=[headers] + all_analysis_results)
            logging.info("‚úÖ‚úÖ‚úÖ –£–°–ü–ï–•! –õ–∏—Å—Ç 'Analysis' –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω –∏ –æ–±–Ω–æ–≤–ª–µ–Ω.")
        except Exception as e:
            logging.error(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ 'Analysis': {e}", exc_info=True)

    logging.info("--- üèÅ –†–ê–ë–û–¢–ê –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê –ó–ê–í–ï–†–®–ï–ù–ê üèÅ ---")

if __name__ == "__main__":
    main_analyzer()